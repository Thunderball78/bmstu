{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement WordNetLemmatizer (from versions: none)\n",
      "ERROR: No matching distribution found for WordNetLemmatizer\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id', 'contract_number', 'object_name', 'object_code', 'cost', 'contract_execution_days']\n",
    "dtypes = {'id': lambda x: pd.to_numeric(x, errors=\"coerce\"), \n",
    "          'contract_number': str, \n",
    "          'object_name': str,\n",
    "          'object_code': str, \n",
    "          'cost':lambda x: pd.to_numeric(x, errors=\"coerce\"), \n",
    "          'contract_execution_days': lambda x: pd.to_numeric(x, errors=\"coerce\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900153, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/dataset_filter.csv', sep=';', low_memory=False, on_bad_lines='skip')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\head\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\head\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\head\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\head\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\head\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получим стоп слова русского языка\n",
    "import nltk\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получим знаки пунктуации\n",
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём список шума из стопслов и знаков пунктуации\n",
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contract_number</th>\n",
       "      <th>object_name</th>\n",
       "      <th>object_code</th>\n",
       "      <th>cost</th>\n",
       "      <th>contract_execution_days</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137439</td>\n",
       "      <td>2590407717221000153</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.9</td>\n",
       "      <td>539265.60</td>\n",
       "      <td>29</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137528</td>\n",
       "      <td>2590421285821000134</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.9</td>\n",
       "      <td>2299469.98</td>\n",
       "      <td>122</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137529</td>\n",
       "      <td>2590421285821000135</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.9</td>\n",
       "      <td>1898111.58</td>\n",
       "      <td>122</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137827</td>\n",
       "      <td>2590500335021000676</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.3</td>\n",
       "      <td>341914.00</td>\n",
       "      <td>199</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138113</td>\n",
       "      <td>2590615606021000060</td>\n",
       "      <td>Услуги в области архитектуры и инженерно-техни...</td>\n",
       "      <td>71.1</td>\n",
       "      <td>1714352.37</td>\n",
       "      <td>383</td>\n",
       "      <td>[услуги, в, области, архитектуры, и, инженерно...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      contract_number  \\\n",
       "0  137439  2590407717221000153   \n",
       "1  137528  2590421285821000134   \n",
       "2  137529  2590421285821000135   \n",
       "3  137827  2590500335021000676   \n",
       "4  138113  2590615606021000060   \n",
       "\n",
       "                                         object_name  object_code        cost  \\\n",
       "0             Работы строительные специализированные         43.9   539265.60   \n",
       "1             Работы строительные специализированные         43.9  2299469.98   \n",
       "2             Работы строительные специализированные         43.9  1898111.58   \n",
       "3             Работы строительные специализированные         43.3   341914.00   \n",
       "4  Услуги в области архитектуры и инженерно-техни...         71.1  1714352.37   \n",
       "\n",
       "   contract_execution_days                                               data  \n",
       "0                       29         [работы, строительные, специализированные]  \n",
       "1                      122         [работы, строительные, специализированные]  \n",
       "2                      122         [работы, строительные, специализированные]  \n",
       "3                      199         [работы, строительные, специализированные]  \n",
       "4                      383  [услуги, в, области, архитектуры, и, инженерно...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 ['работ', 'строительн', 'специализирова']\n",
       "1                 ['работ', 'строительн', 'специализирова']\n",
       "2                 ['работ', 'строительн', 'специализирова']\n",
       "3                 ['работ', 'строительн', 'специализирова']\n",
       "4         ['услуг', 'област', 'архитектур', 'проектирова...\n",
       "                                ...                        \n",
       "551495            ['работ', 'строительн', 'специализирова']\n",
       "551496            ['работ', 'строительн', 'специализирова']\n",
       "551497            ['работ', 'строительн', 'специализирова']\n",
       "551498    ['услуг', 'област', 'архитектур', 'проектирова...\n",
       "551499                ['здан', 'работ', 'возведен', 'здан']\n",
       "Name: data_final, Length: 551500, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import wordnet as wn  # Проверить необходимость\n",
    "\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "word_lemmatized = WordNetLemmatizer()\n",
    "\n",
    "for index, entry in zip(df[\"data\"].index, df[\"data\"]):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words(\"russian\") and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            word_final = stemmer.stem(word_final)\n",
    "            final_words.append(word_final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    df.loc[index, \"data_final\"] = str(final_words)\n",
    "    \n",
    "df[\"data_final\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contract_number</th>\n",
       "      <th>object_name</th>\n",
       "      <th>object_code</th>\n",
       "      <th>cost</th>\n",
       "      <th>contract_execution_days</th>\n",
       "      <th>data</th>\n",
       "      <th>data_final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137439</td>\n",
       "      <td>2590407717221000153</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.9</td>\n",
       "      <td>539265.60</td>\n",
       "      <td>29</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137528</td>\n",
       "      <td>2590421285821000134</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.9</td>\n",
       "      <td>2299469.98</td>\n",
       "      <td>122</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137529</td>\n",
       "      <td>2590421285821000135</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.9</td>\n",
       "      <td>1898111.58</td>\n",
       "      <td>122</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137827</td>\n",
       "      <td>2590500335021000676</td>\n",
       "      <td>Работы строительные специализированные</td>\n",
       "      <td>43.3</td>\n",
       "      <td>341914.00</td>\n",
       "      <td>199</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "      <td>[работы, строительные, специализированные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138113</td>\n",
       "      <td>2590615606021000060</td>\n",
       "      <td>Услуги в области архитектуры и инженерно-техни...</td>\n",
       "      <td>71.1</td>\n",
       "      <td>1714352.37</td>\n",
       "      <td>383</td>\n",
       "      <td>[услуги, в, области, архитектуры, и, инженерно...</td>\n",
       "      <td>['услуг', 'област', 'архитектур', 'проектирова...</td>\n",
       "      <td>[услуги, в, области, архитектуры, и, инженерно...</td>\n",
       "      <td>[услуги, в, области, архитектуры, и, инженерно...</td>\n",
       "      <td>[услуги, области, архитектуры, инженерно-техни...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      contract_number  \\\n",
       "0  137439  2590407717221000153   \n",
       "1  137528  2590421285821000134   \n",
       "2  137529  2590421285821000135   \n",
       "3  137827  2590500335021000676   \n",
       "4  138113  2590615606021000060   \n",
       "\n",
       "                                         object_name  object_code        cost  \\\n",
       "0             Работы строительные специализированные         43.9   539265.60   \n",
       "1             Работы строительные специализированные         43.9  2299469.98   \n",
       "2             Работы строительные специализированные         43.9  1898111.58   \n",
       "3             Работы строительные специализированные         43.3   341914.00   \n",
       "4  Услуги в области архитектуры и инженерно-техни...         71.1  1714352.37   \n",
       "\n",
       "   contract_execution_days                                               data  \\\n",
       "0                       29         [работы, строительные, специализированные]   \n",
       "1                      122         [работы, строительные, специализированные]   \n",
       "2                      122         [работы, строительные, специализированные]   \n",
       "3                      199         [работы, строительные, специализированные]   \n",
       "4                      383  [услуги, в, области, архитектуры, и, инженерно...   \n",
       "\n",
       "                                          data_final  \\\n",
       "0          ['работ', 'строительн', 'специализирова']   \n",
       "1          ['работ', 'строительн', 'специализирова']   \n",
       "2          ['работ', 'строительн', 'специализирова']   \n",
       "3          ['работ', 'строительн', 'специализирова']   \n",
       "4  ['услуг', 'област', 'архитектур', 'проектирова...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0         [работы, строительные, специализированные]   \n",
       "1         [работы, строительные, специализированные]   \n",
       "2         [работы, строительные, специализированные]   \n",
       "3         [работы, строительные, специализированные]   \n",
       "4  [услуги, в, области, архитектуры, и, инженерно...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0         [работы, строительные, специализированные]   \n",
       "1         [работы, строительные, специализированные]   \n",
       "2         [работы, строительные, специализированные]   \n",
       "3         [работы, строительные, специализированные]   \n",
       "4  [услуги, в, области, архитектуры, и, инженерно...   \n",
       "\n",
       "                                               final  \n",
       "0         [работы, строительные, специализированные]  \n",
       "1         [работы, строительные, специализированные]  \n",
       "2         [работы, строительные, специализированные]  \n",
       "3         [работы, строительные, специализированные]  \n",
       "4  [услуги, области, архитектуры, инженерно-техни...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "data = df['object_name'].values.astype('U')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#df[\"data\"] = [word_tokenize(entry.lower()) for entry in df[\"object_name\"]]\n",
    "\n",
    "# получаем список токенов\n",
    "df['tokens'] = [word_tokenize(text.lower(), language=\"russian\") for text in data]\n",
    "\n",
    "# лематизируем полученные токены\n",
    "lemma = WordNetLemmatizer()\n",
    "df['lemma'] = [[lemma.lemmatize(token) for token in text] for text in df['tokens']]\n",
    "#df['lemma'] = lemma.lemmatize(df['tokens'].values)\n",
    "\n",
    "# Очищаем от шума\n",
    "df['final']=[[token for token in text if token not in noise] for text in df['lemma']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = df[['id',\t'contract_number',\t'object_name',\t'object_code',\t'cost',\t'contract_execution_days',\t'data',\t'data_final']]\n",
    "\n",
    "df_save .to_csv('datasets/dataset_nlp.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['услуг', 'област', 'архитектур', 'проектирова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551495</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551496</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551497</th>\n",
       "      <td>['работ', 'строительн', 'специализирова']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551498</th>\n",
       "      <td>['услуг', 'област', 'архитектур', 'проектирова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551499</th>\n",
       "      <td>['здан', 'работ', 'возведен', 'здан']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data_final\n",
       "0               ['работ', 'строительн', 'специализирова']\n",
       "1               ['работ', 'строительн', 'специализирова']\n",
       "2               ['работ', 'строительн', 'специализирова']\n",
       "3               ['работ', 'строительн', 'специализирова']\n",
       "4       ['услуг', 'област', 'архитектур', 'проектирова...\n",
       "...                                                   ...\n",
       "551495          ['работ', 'строительн', 'специализирова']\n",
       "551496          ['работ', 'строительн', 'специализирова']\n",
       "551497          ['работ', 'строительн', 'специализирова']\n",
       "551498  ['услуг', 'област', 'архитектур', 'проектирова...\n",
       "551499              ['здан', 'работ', 'возведен', 'здан']\n",
       "\n",
       "[551500 rows x 1 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "data = df['data'], \n",
    "y = df[\"object_code\"], \n",
    "data_final = df[['data_final']]\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagwords = CountVectorizer(max_features=1500,  min_df=1, max_df=1, stop_words=stopwords.words('russian')) \n",
    "bagwords_X = bagwords.fit_transform(data_final).toarray()\n",
    "bagwords_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mn:\\final-task-bmstu\\4_step_nlp.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bagwords_X_train, bagwords_X_test, bagwords_y_train, bagwords_y_test \u001b[39m=\u001b[39m train_test_split(bagwords_X,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                                                                         y,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                                                         test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                                                         random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2646\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2648\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2649\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2651\u001b[0m )\n\u001b[0;32m   2653\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2654\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2305\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2302\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2304\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2308\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2309\u001b[0m     )\n\u001b[0;32m   2311\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "bagwords_X_train, bagwords_X_test, bagwords_y_train, bagwords_y_test = train_test_split(bagwords_X,\n",
    "                                                                                        y,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bagwords_X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\final-task-bmstu\\4_step_nlp.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m naive_bayes \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bagwords_nb \u001b[39m=\u001b[39m naive_bayes\u001b[39m.\u001b[39mfit(bagwords_X_train, bagwords_y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bagwords_nb_predictions \u001b[39m=\u001b[39m bagwords_nb\u001b[39m.\u001b[39mpredict(bagwords_X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m bagwords_nb_f1 \u001b[39m=\u001b[39m f1_score(bagwords_y_test, bagwords_nb_predictions, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bagwords_X_train' is not defined"
     ]
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "bagwords_nb = naive_bayes.fit(bagwords_X_train, bagwords_y_train)\n",
    "bagwords_nb_predictions = bagwords_nb.predict(bagwords_X_test)\n",
    "\n",
    "bagwords_nb_f1 = f1_score(bagwords_y_test, bagwords_nb_predictions, average='weighted')\n",
    "bagwords_nb_precision = precision_score(bagwords_y_test, bagwords_nb_predictions, average='weighted')\n",
    "print(f'F-мера модели \"Мешок слов\": {round(bagwords_nb_f1, 3)}, точность: {round(bagwords_nb_precision, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mn:\\final-task-bmstu\\4_step_nlp.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_score, f1_score\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m1700\u001b[39m, min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, max_df\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, stop_words\u001b[39m=\u001b[39mstopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39mrussian\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tfidf_X \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform(data_final)\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test \u001b[39m=\u001b[39m train_test_split(tfidf_X,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                                                             y,\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                                             test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                                                             random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/final-task-bmstu/4_step_nlp.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m tfidf_nb \u001b[39m=\u001b[39m naive_bayes\u001b[39m.\u001b[39mfit(tfidf_X_train, tfidf_y_train)\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(max_features=1700, min_df=5, max_df=0.7, stop_words=stopwords.words('russian'))\n",
    "tfidf_X = tfidf.fit_transform(data_final).toarray()\n",
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(tfidf_X,\n",
    "                                                                            y,\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=0)\n",
    "tfidf_nb = naive_bayes.fit(tfidf_X_train, tfidf_y_train)\n",
    "tfidf_nb_predictions = tfidf_nb.predict(tfidf_X_test)\n",
    "tfidf_nb_f1 = f1_score(tfidf_y_test, tfidf_nb_predictions, average='weighted')\n",
    "tfidf_nb_precision = precision_score(tfidf_y_test, tfidf_nb_predictions, average='weighted')\n",
    "print(f'F-мера модели TF-IDF: {round(tfidf_nb_f1, 3)}, точность: {round(tfidf_nb_precision, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(random_state=1, max_iter=10)\n",
    "tfidf_nb = model.fit(tfidf_X_train, tfidf_y_train)\n",
    "tfidf_nb_predictions = model.predict(tfidf_X_test)\n",
    "tfidf_nb_f1 = f1_score(tfidf_y_test, tfidf_nb_predictions, average='weighted')\n",
    "tfidf_nb_precision = precision_score(tfidf_y_test, tfidf_nb_predictions, average='weighted')\n",
    "print(f'F-мера модели TF-IDF: {round(tfidf_nb_f1, 3)}, точность: {round(tfidf_nb_precision, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
